from googleapiclient.http import MediaIoBaseDownload
from googleapiclient.discovery import build
from google.oauth2 import service_account
import streamlit as st
import pandas as pd
import re
import io
from collections import Counter
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive
import tempfile
import json
import os
import plotly.express as px

# ----- Konfiguration ----
st.set_page_config(layout="wide")
st.title("üìö Wortanalyse in Textdateien")
FOLDER_ID = "1-r8Qj_E_SoJEpLzpQTkIho2-GudLlIN7"

# ----- Stoppliste laden -----
STOPWORD_FILE = "stopwords.txt"
try:
    with open(STOPWORD_FILE, encoding="utf-8") as f:
        stopwords = set(line.strip().lower() for line in f if line.strip())
except FileNotFoundError:
    stopwords = set()
    st.warning("‚ö†Ô∏è Keine g√ºltige Stoppliste gefunden. Es werden keine Stoppw√∂rter gefiltert.")

# ----- Google Drive Zugriff -----

@st.cache_resource
def load_files_from_drive():
    try:
        credentials = service_account.Credentials.from_service_account_info(
            dict(st.secrets["google"]),
            scopes=["https://www.googleapis.com/auth/drive.readonly"]
        )
        service = build("drive", "v3", credentials=credentials)

        # Suche nach .txt-Dateien im Ordner
        query = f"'{FOLDER_ID}' in parents and mimeType='text/plain' and trashed=false"
        results = service.files().list(q=query, fields="files(id, name)").execute()
        files = results.get("files", [])

        texts = {}
        for file in files:
            file_id = file["id"]
            file_name = file["name"]
            request = service.files().get_media(fileId=file_id)
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, request)
            done = False
            while not done:
                status, done = downloader.next_chunk()
            fh.seek(0)
            texts[file_name] = fh.read().decode("utf-8")

        return texts

    except Exception as e:
        st.error(f"‚ùå Fehler beim Laden von Dateien: {e}")
        return {}


# ----- Text-Bereinigung -----
def clean_text(text):
    text = text.lower()
    text = re.sub(r"[\W_]+", " ", text)
    return text

# ----- Hauptverarbeitung -----
texts = load_files_from_drive()

if texts:
    book_stats = []
    global_word_counter = Counter()
    new_words_set = set()
    cleaned_texts = {}

    for idx, (file_name, raw_text) in enumerate(texts.items(), start=1):
        cleaned = clean_text(raw_text)
        cleaned_texts[file_name] = cleaned

        words = cleaned.split()
        filtered_words = [w for w in words if w not in stopwords]
        word_count = len(words)
        unique_words = set(filtered_words)
        new_words = unique_words - new_words_set
        new_words_set.update(unique_words)



        book_stats.append({
            "Dateiname": file_name,
            "W√∂rter gesamt": word_count,
            "Einzigartige W√∂rter": len(unique_words),
            "Neue W√∂rter (kumulativ)": len(new_words)
        })

        global_word_counter.update(filtered_words)

    # ---- Statistiken anzeigen ----
    df_stats = pd.DataFrame(book_stats)
    st.subheader("üìà B√ºchstatistiken")

    # ---- Allgemeine Statistik ----
    st.header("üìÑ Allgemeine Statistik")

    col1, col2, col3 = st.columns(3)
    col1.metric("üéß Anzahl der B√ºcher", len(texts))
    col2.metric("Ô∏èüî§ W√∂rter insgesamt", f"{sum(global_word_counter.values()):,}".replace(",", "."))
    col3.metric("üî§ Verschiedene W√∂rter insgesamt", f"{len(global_word_counter):,}".replace(",", "."))

    fig_words = px.line(df_stats.reset_index(), x=df_stats.index + 1, y="W√∂rter gesamt", markers=True,
                        labels={"index": "Episode", "W√∂rter gesamt": "W√∂rter"},
                        title="üìä W√∂rter pro Episode")
    st.plotly_chart(fig_words, use_container_width=True)

    fig_unique = px.line(df_stats.reset_index(), x=df_stats.index + 1, y="Einzigartige W√∂rter", markers=True,
                         labels={"index": "Episode", "Einzigartige W√∂rter": "Einzigartige W√∂rter"},
                         title="üß† Einzigartige W√∂rter pro Episode")
    st.plotly_chart(fig_unique, use_container_width=True)


    # ---- Phrasensuche ----
    st.subheader("üîç Phrasensuche")
    phrase_input = st.text_area("Gib eine oder mehrere Wortgruppen (durch Kommas getrennt) ein, z.‚ÄØB.: feuer, schwarzer rauch")

    if phrase_input:
        phrases = [p.strip().lower() for p in phrase_input.split(",") if p.strip()]
        data = []

        for i, (file_name, cleaned) in enumerate(cleaned_texts.items(), start=1):
            for phrase in phrases:
                count = cleaned.count(" " + phrase + " ")
                data.append({"Episode": i, "Dateiname": file_name, "Phrase": phrase, "Anzahl": count})

        df_phrases = pd.DataFrame(data)
        fig_phrases = px.line(df_phrases, x="Episode", y="Anzahl", color="Phrase", markers=True,
                              title="üìà H√§ufigkeit der gew√§hlten Phrasen √ºber alle Episoden",
                              labels={"Anzahl": "Anzahl", "Episode": "Episode"})
        st.plotly_chart(fig_phrases, use_container_width=True)

    # ---- H√§ufigste W√∂rter in Episoden anzeigen ----
    st.subheader("ü•á Top 10 Episoden mit h√§ufigster Verwendung ausgew√§hlter W√∂rter")

    # Eingabe ausgew√§hlter W√∂rter
    top_words_input = st.text_input("Gib bis zu 5 W√∂rter ein, die analysiert werden sollen (durch Kommas getrennt)",
                                    value="cent, m√ºnze, eimer")

    selected_words = [w.strip().lower() for w in top_words_input.split(",") if w.strip()][:5]

    if selected_words:
        top_data = []

        for i, (file_name, cleaned) in enumerate(cleaned_texts.items(), start=1):
            word_counts = {word: cleaned.split().count(word) for word in selected_words}
            for word, count in word_counts.items():
                top_data.append({
                    "Episode": f"Episode {i}",
                    "Wort": word,
                    "Anzahl": count
                })

        df_top = pd.DataFrame(top_data)
        top_episodes = (
            df_top.groupby("Episode")["Anzahl"].sum()
            .sort_values(ascending=False)
            .head(10)
            .index
        )
        df_top_filtered = df_top[df_top["Episode"].isin(top_episodes)]

        fig_top_words = px.bar(df_top_filtered,
                               x="Anzahl", y="Episode", color="Wort", orientation="h",
                               title="ü•á Top 10 Episoden mit h√§ufigster Verwendung ausgew√§hlter W√∂rter",
                               labels={"Anzahl": "Anzahl", "Episode": "Episode (nur Top 10)"}
                               )

        st.plotly_chart(fig_top_words, use_container_width=True)

    # ---- Top-W√∂rter anzeigen ----
    st.subheader("üèÜ Top 20 h√§ufigste W√∂rter insgesamt (ohne Stoppw√∂rter)")
    most_common_df = pd.DataFrame(global_word_counter.most_common(20), columns=["Wort", "Anzahl"])
    fig_common = px.bar(most_common_df, x="Wort", y="Anzahl", title="üèÖ H√§ufigste W√∂rter", text="Anzahl")
    st.plotly_chart(fig_common, use_container_width=True)

    # ---- Download ----
    csv = df_stats.to_csv(index=False).encode('utf-8')
    st.download_button("üì• Lade Statistiken als CSV herunter", csv, "buchstatistiken.csv", "text/csv")
else:
    st.warning("‚ùó Keine Textdateien gefunden oder Ordner-ID ist ung√ºltig.")
